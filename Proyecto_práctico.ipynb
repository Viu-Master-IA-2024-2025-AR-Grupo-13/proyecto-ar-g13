{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#1. Ejecutar esta celda individualmente\n#2. Reiniciar entorno cuando acabe (Run -> Restart and clear cell outputs )\n#3. Ejecutar desde la siguiente celda en adelante\n\n%pip install gym==0.17.3\n%pip install git+https://github.com/Kojoley/atari-py.git\n%pip install keras-rl2==1.0.5\n%pip install tensorflow==2.12","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import division\n\nfrom PIL import Image\nimport numpy as np\nimport gym\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\nfrom tensorflow.keras.optimizers.legacy import Adam\nimport tensorflow.keras.backend as K\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory\nfrom rl.core import Processor\nfrom rl.callbacks import FileLogger, ModelIntervalCheckpoint\n\nimport os\nimport time\nimport json\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:33:01.679499Z","iopub.execute_input":"2025-06-21T06:33:01.679796Z","iopub.status.idle":"2025-06-21T06:33:05.108042Z","shell.execute_reply.started":"2025-06-21T06:33:01.679774Z","shell.execute_reply":"2025-06-21T06:33:05.107427Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"INPUT_SHAPE = (84, 84)\nWINDOW_LENGTH = 4\n\nenv_name = 'SpaceInvaders-v0'\nenv = gym.make(env_name)\n\nnp.random.seed(123)\nenv.seed(123)\nnb_actions = env.action_space.n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:33:05.974595Z","iopub.execute_input":"2025-06-21T06:33:05.975032Z","iopub.status.idle":"2025-06-21T06:33:06.274875Z","shell.execute_reply.started":"2025-06-21T06:33:05.975010Z","shell.execute_reply":"2025-06-21T06:33:06.274178Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class AtariProcessor(Processor):\n    def process_observation(self, observation):\n        assert observation.ndim == 3  # (height, width, channel)\n        img = Image.fromarray(observation)\n        img = img.resize(INPUT_SHAPE).convert('L')\n        processed_observation = np.array(img)\n        assert processed_observation.shape == INPUT_SHAPE\n        return processed_observation.astype('uint8')\n\n    def process_state_batch(self, batch):\n        processed_batch = batch.astype('float32') / 255.\n        return processed_batch\n\n    def process_reward(self, reward):\n        return np.clip(reward, -1., 1.)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:33:09.291383Z","iopub.execute_input":"2025-06-21T06:33:09.291960Z","iopub.status.idle":"2025-06-21T06:33:09.297354Z","shell.execute_reply.started":"2025-06-21T06:33:09.291925Z","shell.execute_reply":"2025-06-21T06:33:09.296553Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE))\nmodel.add(Convolution2D(32, (8, 8), strides=(4, 4)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, (4, 4), strides=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, (3, 3), strides=(1, 1)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:33:15.203759Z","iopub.execute_input":"2025-06-21T06:33:15.204362Z","iopub.status.idle":"2025-06-21T06:33:15.312945Z","shell.execute_reply.started":"2025-06-21T06:33:15.204338Z","shell.execute_reply":"2025-06-21T06:33:15.312406Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ===================== CONFIGURACIÓN =====================\nblock_steps = 10_000 ###########\nlog_path = \"/kaggle/working\"\n\nweights_prefix = f\"{log_path}/dqn_{env_name}_weights\"\nstate_prefix = f\"{log_path}/dqn_state\"\nlog_prefix = f\"{log_path}/log_block\"\n\n# ===================== DETECCIÓN DE CHECKPOINT =====================\npattern = re.compile(rf'dqn_{env_name}_weights_(\\d+).h5f')\ncheckpoint_files = [f for f in os.listdir(log_path) if pattern.match(f)]\n\nif checkpoint_files:\n    last_step = max(int(pattern.match(f).group(1)) for f in checkpoint_files)\n    weights_path = f\"{weights_prefix}_{last_step}.h5f\"\n    state_path = f\"{state_prefix}_{last_step}.json\"\n    with open(state_path, \"r\") as f:\n        state = json.load(f)\n    restored_step = state.get(\"step\", 0)\n    nb_warmup = 1000 # warmup menor, solo para que el buffer no esté vacío\n    print(f\" Retomando desde paso {restored_step}\")\nelse:\n    last_step = 0\n    restored_step = 0\n    nb_warmup = 5000 ###########\n    print(\" Entrenamiento nuevo\")\n\n# ===================== CONFIGURAR COMPONENTES =====================\nmemory = SequentialMemory(limit=1_000_000, window_length=WINDOW_LENGTH)\n\npolicy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n                              value_max=1.0, value_min=0.1, value_test=0.05,\n                              nb_steps=1_000_000)\n\ndqn = DQNAgent(model=model,\n               nb_actions=nb_actions,\n               policy=policy,\n               memory=memory,\n               processor=AtariProcessor(),\n               nb_steps_warmup=nb_warmup,\n               gamma=0.99,\n               target_model_update=10000,\n               train_interval=4,\n               delta_clip=1.0)\n\ndqn.compile(Adam(learning_rate=0.00025), metrics=['mae'])\n\nif last_step > 0:\n    dqn.load_weights(weights_path)\n    dqn.step = restored_step\n\n# ===================== ENTRENAR BLOQUE =====================\ncurrent_step = last_step + block_steps\ncurrent_block = current_step // block_steps\nprint(f\"\\n Entrenando bloque {current_block} (pasos {last_step + 1} → {current_step})\")\n\nlog_file = f\"{log_prefix}_{current_block:03d}.json\"\nlog_callback = FileLogger(log_file, interval=100)\n\ndqn.fit(env, nb_steps=block_steps, visualize=False, verbose=2, callbacks=[log_callback])\n\n# ===================== GUARDAR PROGRESO =====================\nweights_out = f\"{weights_prefix}_{current_step}.h5f\"\nstate_out = f\"{state_prefix}_{current_step}.json\"\n\ndqn.save_weights(weights_out, overwrite=True)\nwith open(state_out, \"w\") as f:\n    json.dump({\n        \"step\": int(dqn.step)\n    }, f)\n\nprint(f\"\\n Bloque {current_block} completado - step: {dqn.step}, epsilon: {dqn.policy.inner_policy.eps:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:55:04.993298Z","iopub.execute_input":"2025-06-21T06:55:04.993597Z"}},"outputs":[{"name":"stdout","text":" Entrenamiento nuevo\n\n Entrenando bloque 1 (pasos 1 → 10000)\nTraining for 10000 steps ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  updates=self.state_updates,\n","output_type":"stream"},{"name":"stdout","text":" 1639/10000: episode: 1, duration: 6.914s, episode steps: 1639, steps per second: 237, episode reward: 25.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n 2172/10000: episode: 2, duration: 2.172s, episode steps: 533, steps per second: 245, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n 3143/10000: episode: 3, duration: 3.914s, episode steps: 971, steps per second: 248, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n 4420/10000: episode: 4, duration: 5.119s, episode steps: 1277, steps per second: 249, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  updates=self.state_updates,\n","output_type":"stream"},{"name":"stdout","text":" 5248/10000: episode: 5, duration: 11.925s, episode steps: 828, steps per second:  69, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.008325, mae: 0.279835, mean_q: 0.368011, mean_eps: 0.995388\n 6279/10000: episode: 6, duration: 33.534s, episode steps: 1031, steps per second:  31, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.008283, mae: 0.286480, mean_q: 0.370034, mean_eps: 0.994814\n 7145/10000: episode: 7, duration: 28.344s, episode steps: 866, steps per second:  31, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.007154, mae: 0.292439, mean_q: 0.380669, mean_eps: 0.993959\n 7618/10000: episode: 8, duration: 15.204s, episode steps: 473, steps per second:  31, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.005792, mae: 0.296186, mean_q: 0.384979, mean_eps: 0.993356\n 7990/10000: episode: 9, duration: 12.363s, episode steps: 372, steps per second:  30, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.006584, mae: 0.296664, mean_q: 0.384091, mean_eps: 0.992976\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Testing part to calculate the mean reward\ndqn.load_weights(final_weights)\ndqn.test(env, nb_episodes=10, visualize=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%ls -l /kaggle/working\n%rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:55:01.428087Z","iopub.execute_input":"2025-06-21T06:55:01.428593Z","iopub.status.idle":"2025-06-21T06:55:01.745676Z","shell.execute_reply.started":"2025-06-21T06:55:01.428562Z","shell.execute_reply":"2025-06-21T06:55:01.744618Z"}},"outputs":[{"name":"stdout","text":"total 13220\n-rw-r--r-- 1 root root     133 Jun 21 06:54 checkpoint\n-rw-r--r-- 1 root root 6750266 Jun 21 06:54 dqn_SpaceInvaders-v0_weights_10000.h5f.data-00000-of-00001\n-rw-r--r-- 1 root root     781 Jun 21 06:54 dqn_SpaceInvaders-v0_weights_10000.h5f.index\n-rw-r--r-- 1 root root 6750266 Jun 21 06:54 dqn_SpaceInvaders-v0_weights_20000.h5f.data-00000-of-00001\n-rw-r--r-- 1 root root     781 Jun 21 06:54 dqn_SpaceInvaders-v0_weights_20000.h5f.index\n-rw-r--r-- 1 root root      15 Jun 21 06:54 dqn_state_10000.json\n-rw-r--r-- 1 root root      14 Jun 21 06:54 dqn_state_20000.json\n-rw-r--r-- 1 root root    1562 Jun 21 06:54 log_block_001.json\n-rw-r--r-- 1 root root     243 Jun 21 06:54 log_block_002.json\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nimport json\nimport matplotlib.pyplot as plt\n\n# Ruta al archivo log\nlog_path = f'/kaggle/working/dqn_{env_name}_log.json'\n\n# Cargar datos\nwith open(log_path, 'r') as f:\n    history = json.load(f)\n\n# Extraer métricas\nrewards = history['episode_reward']\nlosses = history.get('loss', [])\nmean_qs = history.get('mean_q', [])\nmean_eps = history.get('mean_eps', [])\n\n# === Función para media acumulada ===\ndef cumulative_average(data):\n    return np.cumsum(data) / np.arange(1, len(data) + 1)\n\n# === Gráfica de recompensas ===\nplt.figure(figsize=(12, 4))\nplt.plot(rewards, label='Reward')\nplt.plot(cumulative_average(rewards), label='Mean Reward (cumulative)', color='red', linewidth=2)\nplt.title('Recompensa por episodio')\nplt.xlabel('Episodio')\nplt.ylabel('Recompensa')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# === Gráfica de pérdida ===\nif losses:\n    plt.figure(figsize=(12, 4))\n    plt.plot(losses, label='Loss')\n    plt.plot(cumulative_average(losses), label='Mean Loss (cumulative)', color='red', linewidth=2)\n    plt.title('Pérdida (loss) durante el entrenamiento')\n    plt.xlabel('Episodio')\n    plt.ylabel('Loss')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n# === Gráfica de mean_q ===\nif mean_qs:\n    plt.figure(figsize=(12, 4))\n    plt.plot(mean_qs, label='Mean Q')\n    plt.plot(cumulative_average(mean_qs), label='Mean Q (cumulative)', color='red', linewidth=2)\n    plt.title('Valor medio Q (mean_q)')\n    plt.xlabel('Episodio')\n    plt.ylabel('Q')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n# === Gráfica de epsilon ===\nif mean_eps:\n    plt.figure(figsize=(12, 4))\n    plt.plot(mean_eps, label='Epsilon')\n    plt.plot(cumulative_average(mean_eps), label='Mean Epsilon (cumulative)', color='red', linewidth=2)\n    plt.title('Exploración (epsilon) por episodio')\n    plt.xlabel('Episodio')\n    plt.ylabel('Epsilon')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}