{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#1. Ejecutar esta celda individualmente\n#2. Reiniciar entorno cuando acabe (Run -> Restart and clear cell outputs )\n#3. Ejecutar desde la siguiente celda en adelante\n\n%pip install gym==0.17.3\n%pip install git+https://github.com/Kojoley/atari-py.git\n%pip install keras-rl2==1.0.5\n%pip install tensorflow==2.12","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import division\n\nfrom PIL import Image\nimport numpy as np\nimport gym\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\nfrom tensorflow.keras.optimizers.legacy import Adam\nimport tensorflow.keras.backend as K\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory\nfrom rl.core import Processor\nfrom rl.callbacks import FileLogger, ModelIntervalCheckpoint\n\nimport os\nimport time\nimport json\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:33:01.679499Z","iopub.execute_input":"2025-06-21T06:33:01.679796Z","iopub.status.idle":"2025-06-21T06:33:05.108042Z","shell.execute_reply.started":"2025-06-21T06:33:01.679774Z","shell.execute_reply":"2025-06-21T06:33:05.107427Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"INPUT_SHAPE = (84, 84)\nWINDOW_LENGTH = 4\n\nenv_name = 'SpaceInvaders-v0'\nenv = gym.make(env_name)\n\nnp.random.seed(123)\nenv.seed(123)\nnb_actions = env.action_space.n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:33:05.974595Z","iopub.execute_input":"2025-06-21T06:33:05.975032Z","iopub.status.idle":"2025-06-21T06:33:06.274875Z","shell.execute_reply.started":"2025-06-21T06:33:05.975010Z","shell.execute_reply":"2025-06-21T06:33:06.274178Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class AtariProcessor(Processor):\n    def process_observation(self, observation):\n        assert observation.ndim == 3  # (height, width, channel)\n        img = Image.fromarray(observation)\n        img = img.resize(INPUT_SHAPE).convert('L')\n        processed_observation = np.array(img)\n        assert processed_observation.shape == INPUT_SHAPE\n        return processed_observation.astype('uint8')\n\n    def process_state_batch(self, batch):\n        processed_batch = batch.astype('float32') / 255.\n        return processed_batch\n\n    def process_reward(self, reward):\n        return np.clip(reward, -1., 1.)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:33:09.291383Z","iopub.execute_input":"2025-06-21T06:33:09.291960Z","iopub.status.idle":"2025-06-21T06:33:09.297354Z","shell.execute_reply.started":"2025-06-21T06:33:09.291925Z","shell.execute_reply":"2025-06-21T06:33:09.296553Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Permute((2, 3, 1), input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE))\nmodel.add(Convolution2D(32, (8, 8), strides=(4, 4)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, (4, 4), strides=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, (3, 3), strides=(1, 1)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T06:33:15.203759Z","iopub.execute_input":"2025-06-21T06:33:15.204362Z","iopub.status.idle":"2025-06-21T06:33:15.312945Z","shell.execute_reply.started":"2025-06-21T06:33:15.204338Z","shell.execute_reply":"2025-06-21T06:33:15.312406Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ===================== CONFIGURACIÓN =====================\nblock_steps = 100_000\nlog_path = \"/kaggle/working\"\n\nweights_prefix = f\"{log_path}/dqn_{env_name}_weights\"\nstate_prefix = f\"{log_path}/dqn_state\"\nlog_prefix = f\"{log_path}/log_block\"\n\n# ===================== DETECCIÓN DE CHECKPOINT =====================\nimport os, re, json\n\npattern = re.compile(rf'dqn_{env_name}_weights_(\\d+).h5f')\ncheckpoint_files = [f for f in os.listdir(log_path) if pattern.match(f)]\n\nif checkpoint_files:\n    last_step = max(int(pattern.match(f).group(1)) for f in checkpoint_files)\n    weights_path = f\"{weights_prefix}_{last_step}.h5f\"\n    state_path = f\"{state_prefix}_{last_step}.json\"\n    with open(state_path, \"r\") as f:\n        state = json.load(f)\n    restored_step = state.get(\"step\", 0)\n    nb_warmup = 1000  # warmup corto al reanudar\n    print(f\"Retomando desde paso {restored_step}\")\nelse:\n    last_step = 0\n    restored_step = 0\n    nb_warmup = 50000\n    print(\"Entrenamiento nuevo\")\n\n# ===================== CONFIGURAR COMPONENTES =====================\nmemory = SequentialMemory(limit=1_000_000, window_length=WINDOW_LENGTH)\n\npolicy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n                              value_max=1.0, value_min=0.1, value_test=0.05,\n                              nb_steps=1_000_000)\n\ndqn = DQNAgent(model=model,\n               nb_actions=nb_actions,\n               policy=policy,\n               memory=memory,\n               processor=AtariProcessor(),\n               nb_steps_warmup=nb_warmup,\n               gamma=0.99,\n               target_model_update=10000,\n               train_interval=4,\n               delta_clip=1.0)\n\ndqn.compile(Adam(learning_rate=0.00025), metrics=['mae'])\n\n# ===================== RESTAURAR CHECKPOINT =====================\nif last_step > 0:\n    dqn.load_weights(weights_path)\n    dqn.step = restored_step\n\n# ===================== ENTRENAR BLOQUE =====================\ncurrent_step = last_step + block_steps\ncurrent_block = current_step // block_steps\nprint(f\"\\nEntrenando bloque {current_block} (pasos {last_step + 1} → {current_step})\")\n\nlog_file = f\"{log_prefix}_{current_block:03d}.json\"\nlog_callback = FileLogger(log_file, interval=100)\n\ndqn.fit(env, nb_steps=block_steps, visualize=False, verbose=2, callbacks=[log_callback])\n\n# ===================== GUARDAR CHECKPOINT =====================\nweights_out = f\"{weights_prefix}_{current_step}.h5f\"\nstate_out = f\"{state_prefix}_{current_step}.json\"\n\ndqn.save_weights(weights_out, overwrite=True)\nwith open(state_out, \"w\") as f:\n    json.dump({\"step\": int(dqn.step)}, f)\n\nprint(f\"\\nBloque {current_block} completado - step: {dqn.step}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T07:20:23.181997Z","iopub.execute_input":"2025-06-21T07:20:23.182298Z"}},"outputs":[{"name":"stdout","text":"Entrenamiento nuevo\n\nEntrenando bloque 1 (pasos 1 → 100000)\nTraining for 100000 steps ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  updates=self.state_updates,\n","output_type":"stream"},{"name":"stdout","text":"   803/100000: episode: 1, duration: 3.834s, episode steps: 803, steps per second: 209, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n  1194/100000: episode: 2, duration: 1.608s, episode steps: 391, steps per second: 243, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n  2019/100000: episode: 3, duration: 3.349s, episode steps: 825, steps per second: 246, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n  2608/100000: episode: 4, duration: 2.377s, episode steps: 589, steps per second: 248, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n  3201/100000: episode: 5, duration: 2.442s, episode steps: 593, steps per second: 243, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Testing part to calculate the mean reward\ndqn.load_weights(final_weights)\ndqn.test(env, nb_episodes=10, visualize=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%ls -l /kaggle/working\n%rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T07:20:19.175488Z","iopub.execute_input":"2025-06-21T07:20:19.175833Z","iopub.status.idle":"2025-06-21T07:20:19.547399Z","shell.execute_reply.started":"2025-06-21T07:20:19.175804Z","shell.execute_reply":"2025-06-21T07:20:19.546353Z"}},"outputs":[{"name":"stdout","text":"total 0\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import numpy as np\nimport json\nimport matplotlib.pyplot as plt\n\n# Ruta al archivo log\nlog_path = f'/kaggle/working/dqn_{env_name}_log.json'\n\n# Cargar datos\nwith open(log_path, 'r') as f:\n    history = json.load(f)\n\n# Extraer métricas\nrewards = history['episode_reward']\nlosses = history.get('loss', [])\nmean_qs = history.get('mean_q', [])\nmean_eps = history.get('mean_eps', [])\n\n# === Función para media acumulada ===\ndef cumulative_average(data):\n    return np.cumsum(data) / np.arange(1, len(data) + 1)\n\n# === Gráfica de recompensas ===\nplt.figure(figsize=(12, 4))\nplt.plot(rewards, label='Reward')\nplt.plot(cumulative_average(rewards), label='Mean Reward (cumulative)', color='red', linewidth=2)\nplt.title('Recompensa por episodio')\nplt.xlabel('Episodio')\nplt.ylabel('Recompensa')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# === Gráfica de pérdida ===\nif losses:\n    plt.figure(figsize=(12, 4))\n    plt.plot(losses, label='Loss')\n    plt.plot(cumulative_average(losses), label='Mean Loss (cumulative)', color='red', linewidth=2)\n    plt.title('Pérdida (loss) durante el entrenamiento')\n    plt.xlabel('Episodio')\n    plt.ylabel('Loss')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n# === Gráfica de mean_q ===\nif mean_qs:\n    plt.figure(figsize=(12, 4))\n    plt.plot(mean_qs, label='Mean Q')\n    plt.plot(cumulative_average(mean_qs), label='Mean Q (cumulative)', color='red', linewidth=2)\n    plt.title('Valor medio Q (mean_q)')\n    plt.xlabel('Episodio')\n    plt.ylabel('Q')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n# === Gráfica de epsilon ===\nif mean_eps:\n    plt.figure(figsize=(12, 4))\n    plt.plot(mean_eps, label='Epsilon')\n    plt.plot(cumulative_average(mean_eps), label='Mean Epsilon (cumulative)', color='red', linewidth=2)\n    plt.title('Exploración (epsilon) por episodio')\n    plt.xlabel('Episodio')\n    plt.ylabel('Epsilon')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}